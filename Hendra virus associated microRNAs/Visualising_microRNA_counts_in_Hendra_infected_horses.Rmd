---
title: Mining the transcriptome for biomarkers of infection 
subtitle: A call for larger sample sizes 
author:  Marina Alexander
affiliation: Health & Biosecurity| Managing invasive species and Disease
photo: resources/img/Measles.jpg

short_title: Optional short title

output: 
  DSreport::project_summary:
    code_folding: hide
---
```{r setup, include=FALSE}
#Remind R where to look for libraries
.libPaths(c("C:/Users/ale097/Data School/Packages"))
#Load libraries
library(tidyverse)
library(gapminder)
library(kableExtra)
library(knitr)
library(edgeR)
library(limma)
library(cowplot)
library(rlang)

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(
  results = 'asis',
  warning = FALSE,
  message = FALSE,
  fig.align = 'center'
)

read_csv("data/Redlands_counts.csv")   
microRNA_counts <- read_csv("data/Redlands_counts.csv")
read_csv("data/redlands_horse_metadata.csv")                            
redlands_horse_metadata <- read_csv("data/redlands_horse_metadata.csv")

```


# Introduction
Hi my name is Marina. I work on viruses. Before data school I would typically enter my data into Excel or Prism to make bar graphs with errors bars, then communicate these in publications and powerpoint slides. Using R, I can now reproducibly wrangle data imported from Excel, plot with ggplot2 to make really flexible and informative visualisations of the raw data. This has helped me to understand the sources of variation in my data, apply appropriate statistics and make novel visualisations.  

# My Project
My group are looking to find microRNAs that are predictive of viral infection. Such microRNAs could act as early bio-markers of infection. Allowing us to isolate the infected individual before they show signs of infection.  To identify these microRNAs we set up an experiment in which three horses were infected with Hendra virus and blood was taken at day 0, 1, 3, 5 and 7. RNA was extracted and sequenced, many millions of sequencing reads were then aligned to the 800+ currently defined microRNAs in the horse genome. This process generates a table where the number of aligned reads for each microRNA are counted for each sample. My goal was to model a relationship between microRNA expression level and time since infection using multi-linear regression.

<style>
.column-left{
  float: left;
  width: 25%;
  text-align: left;
}
.column-center{
  display: inline-block;
  width: 60%;
  text-align: center;
}
.column-right{
  float: right;
  width: 15%;
  text-align: right;
}
</style>

<div class="column-left">
</br>
I began with two tables, one a count matrix for each microRNA and the other a table containing metadata. 
</br>
</br>
Just be looking at these tables, there are some challenging apsects to this data.
</br>
* 800+ microRNAs
</br>
* microRNAs with zero counts
</br>
* Sample 6 did not work 
</br>
* High dynamic range of counts 
</div>
<div class="column-center">
Snippet of microRNA counts
```{r counts_table, out.width='100%', echo = FALSE}
knitr::kable(head(microRNA_counts[1:7], n =6), format = "html") %>% 
  kable_styling("striped", full_width = FALSE)

```
</div>
<div class="column-right">
Snippet of metadata
```{r metadata_table, out.width = '100%', echo = FALSE}

knitr::kable(head(redlands_horse_metadata, n = 6 ), format = "html") %>% 
  kable_styling("striped", full_width = FALSE)

```
</div>
```{r tidying data for plotting, out.width = '100%', results = 'hide', echo=FALSE}
redlands_horse_metadata_long <- redlands_horse_metadata %>% 
  mutate(day = sub("d","", condition )) %>%
  mutate(day = as.numeric(day)) %>%
  select(-condition) 

lib_size <- microRNA_counts %>%
  gather(sample, counts, -gene) %>% 
  left_join(redlands_horse_metadata_long, by = "sample") %>% 
  rename(library = sample) %>% 
  mutate(library = sub("s","", library)) %>% 
  mutate(counts_million = counts/1000000) %>% 
  group_by(library) %>% 
  summarise(counts_sum = sum(counts_million))

median <- lib_size %>% 
  filter(library != 6) %>%
  summarise(median = median(counts_sum))

```
## Data clean up
Plotting the total counts for each sample or library we see that they vary considerably around the median for this experiment (`r median(lib_size$counts_sum)` million counts as shown by the red dashed line). To conduct a filtering step we'll need to normalise counts by library size so that sample 11 for example doesn't loose too many microRNAs because the library wasn't as high yeilding as the others. 
```{r plotting library size, out.width = '60%', fig.align = 'center'}

counts_to_plot <- microRNA_counts %>%
  gather(sample, counts, -gene) %>% 
  left_join(redlands_horse_metadata_long, by = "sample") %>% 
  rename(library = sample) %>% 
  mutate(library = sub("s","", library)) %>%
  mutate(counts_million = counts/1000000) %>% 
  full_join(lib_size, by = "library")
  
ggplot(counts_to_plot, aes(y = counts_sum, x = library, color = day, shape = animal)) +
  geom_point(size = 5) +
  geom_hline(yintercept=1.808277, linetype="dashed", color = "red") +
  scale_x_discrete(limits = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)) +
  labs( x = "Library", 
        y = "Library size (million counts)", 
        title = "Library sizes") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = "bold"),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 14, face = "bold"),
        title = element_text(size = 14, face = "bold"))

```

We can convert the raw counts to counts per million (CPM) using the `edgeR` package after removing sample 6. Then we filter the lowly expressed microRNAs,  The `filterByExpr` function used here removes genes based on sequencing depth and experimental design. For this dataset I kept microRNAs with less than 5 cpm in at least 3 samples because we have 3 animals or biological replicates and also removed genes with less than 80 cpm across all samples. Shown here is a density plot of log transformed counts before and after filtering, where the red dahsed line represents the 5 cpm cut off chosen for this dataset. 

```{r filtering out lowly expressed genes, out.width = '60%', fig.align = 'center'}
#creating a DGEList with counts per million after removing s6
microRNA_counts_s6 <- microRNA_counts %>% 
  select(-s6)

redlands_horse_metadata_s6 <- redlands_horse_metadata %>% 
  filter(sample != "s6")

horse_counts <- DGEList(counts = microRNA_counts_s6[, -1], genes = microRNA_counts_s6[, 1], samples = redlands_horse_metadata_s6)

# tidying for plotting
cpm_long <- cpm(horse_counts) %>% 
  as.tibble() %>%
  bind_cols(as.tibble(horse_counts$genes)) %>%
  gather(sample, cpm, -gene)
  
redlands_horse_metadata_s6_long <- 
  redlands_horse_metadata_long %>% 
  filter(sample != "s6")

counts_cpm <- microRNA_counts_s6 %>%
  gather(sample, counts, -gene) %>% 
  left_join(redlands_horse_metadata_s6_long, by = "sample") %>%
  bind_cols(cpm_long) %>%
  select(-sample1, -gene1) %>% 
  rename(library = sample) %>% 
  mutate(library = sub("s","", library)) %>%
  mutate(counts_million = counts/1000000) %>% 
  full_join(lib_size, by = "library") %>% 
  mutate(log_cpm = log(cpm))

min_count <- 10/median

plot_raw <- ggplot(counts_cpm, aes(x = log_cpm, color = library)) +
  geom_density() +
  geom_vline(xintercept = log(5.53), linetype="dashed", color = "red") +
  scale_color_discrete(limits = c(1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15)) +
  labs(title = "Unfiltered microRNAs",
       x = "log-cpm") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = "bold"),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 14, face = "bold"),
        title = element_text(size = 14, face = "bold"),
        plot.margin = margin(6, 0, 6, 0))

# filtering lowly expressed genes
design_matrix <- model.matrix(~ condition + animal, data = redlands_horse_metadata)

gene_filter <- filterByExpr(horse_counts, min.count = 6, min.total.count = 80, design = design_matrix)

horse_filtered <- horse_counts[gene_filter, , keep.lib.sizes = FALSE]

# tidying for plotting
filtered_cpm_long <- cpm(horse_filtered) %>% 
  as.tibble() %>%
  bind_cols(as.tibble(horse_filtered$genes)) %>%
  gather(sample, cpm, -gene)

filtered_cpm <- filtered_cpm_long %>% 
  left_join(cpm_long) %>% 
  left_join(redlands_horse_metadata_s6_long) %>%
  rename(library = sample) %>% 
  mutate(library = sub("s","", library)) %>% 
  mutate(log_filtered_cpm = log(cpm))

plot_filtered <- ggplot(filtered_cpm, aes(x = log_filtered_cpm, color = library)) +
  geom_density() +
  geom_vline(xintercept = log(5.53), linetype="dashed", color = "red") +
  scale_color_discrete(limits = c(1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15)) +
  labs(title = "Filtered microRNAs",
       x = "log-cpm",
       y = "") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = "bold"),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 14, face = "bold"),
        title = element_text(size = 14, face = "bold"),
        plot.margin = margin(6, 0, 6, 0))


plots <- plot_grid(
  plot_raw + theme(legend.position="none"),
  plot_filtered + theme(legend.position="none"),
  hjust = -1,
  nrow = 1
)

legend <- get_legend(
  plot_raw + theme(legend.box.margin = margin(0, 0, 0, 6))
)

plot_grid(plots, legend, rel_widths = c(3, .4))

```

We are left with `r nrow(horse_filtered$genes)` from the original `r nrow(horse_counts$genes)` and now need to perform normalisation. It is assumed that all samples should have a similar range and distribution of expression values. However, during the sample preparation or sequencing process, external factors that are not of biological interest can affect the expression of individual samples. Normalisation is required to ensure that the expression distributions of each sample are similar across the entire experiment. edgeR employs the Trimmed Means of M values (TMM) in which highly expressed genes and those that have a large variation of expression are excluded, whereupon a weighted average of the subset of genes is used to calculate a normalization factor.

```{r TMM normalisation, out.width = '70%', fig.align = 'center'}

horse_norm <- calcNormFactors(horse_filtered, method = "TMM")

norm_cpm_long <- cpm(horse_norm) %>% 
  as.tibble() %>%
  bind_cols(as.tibble(horse_norm$genes)) %>%
  gather(sample, norm_cpm, -gene)

norm_cpm <- norm_cpm_long %>% 
  left_join(cpm_long) %>% 
  left_join(redlands_horse_metadata_s6_long) %>%
  rename(library = sample) %>% 
  mutate(library = sub("s","", library)) %>% 
  mutate(log_cpm = log(cpm)) %>% 
  mutate(log_norm_cpm = log(norm_cpm))

plot_unnorm <- ggplot(norm_cpm, aes(y = log_cpm, x = library)) +
  geom_boxplot() +
  scale_x_discrete(
    limits = c(1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14) ) +
  labs(title = "Unnormalised",
       y = "log-cpm")
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = "bold"),
        title = element_text(size = 14, face = "bold"))


plot_norm <- ggplot(norm_cpm, aes(y = log_norm_cpm, x = library)) +
  geom_boxplot() +
  scale_x_discrete(
    limits = c(1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14)) +
  labs(title = "Normalised",
       y = "") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = "bold"),
        title = element_text(size = 14, face = "bold"))

plot_grid(plot_unnorm, plot_norm)

```

EdgeR uses the negative binomial distribution to deal with count variability when performing multi linear regression. But forst let's do some quality control by performing principle component analysis.  


# My Digital Toolbox

What digital tools have you been using in your project? Which ones have you learned since starting 
Data School?

You can use all the usual R markdown features in writing a project summary, including lists:

* R - dplyr, tidyverse, ggplot,

![](resources/img/tidyverse.png){width=100px}
![](resources/img/edgeR.png){width=270px}
![](https://raw.githubusercontent.com/tidyverse/ggplot2/master/man/figures/logo.png){.pull-right width=100px}
## Favourite tool (optional)

Is there a tool/package/function in particular that you've enjoyed using? Give it a special shout out here.



No prizes for guessing mine:

# My time went ...

What parts of the project took the most time and effort? Were there any surprising challenges you
encountered, and how did you solve them?

# Next steps

What further steps do you wish your project could take? Or are there any new digital skills that you
are keen to develop as a result of your involvement in the Data School?

# My Data School Experience

This poster is mostly about your synthesis project. However we would also like to hear about other
parts of your Data School experience. What aspects of the program did you really enjoy? How have you
been applying the skills you have learned in your daily work? Have you been able to transfer this 
knowledge to your team members? Concrete examples demonstrating this would be useful here
(meetings/talks/collaborations/new roles). Any descriptions of the personal impact the program has 
had are welcome here as well!
